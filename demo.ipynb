{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Strong Mayor Powers Classification Tool - Demo\n",
    "\n",
    "This notebook demonstrates how to use the Strong Mayor Powers classification tool to analyze documents and detect references to Strong Mayor Powers in public comments.\n",
    "\n",
    "## What is this tool?\n",
    "\n",
    "The Strong Mayor Powers Detection Tool is a command-line utility that uses the Google Gemini API to classify documents for the presence or absence of references to \"Strong Mayor Powers\" within public consultation documents. \n",
    "\n",
    "**Strong Mayor Powers** refer to enhanced mayoral authorities introduced in Ontario municipalities, including:\n",
    "- Authority to override certain council decisions with a simple majority vote\n",
    "- Enhanced control over municipal planning and development processes\n",
    "- Greater influence over municipal budget priorities\n",
    "- Streamlined decision-making capabilities for municipal governance\n",
    "- Powers to hire and dismiss certain municipal staff directly\n",
    "\n",
    "## What we'll demonstrate\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Set up the required dependencies\n",
    "2. Create and examine sample documents\n",
    "3. Run the tool in dry-run mode to estimate costs\n",
    "4. Process documents using direct upload (no text extraction)\n",
    "5. Interpret the output\n",
    "\n",
    "**Note**: For the full classification to work, you'll need a Gemini API key. The tool now uses the new Google GenAI API with direct document upload - no more text extraction!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies\n",
    "\n",
    "First, let's import the required libraries and check that our classification script is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if classify.py exists\n",
    "if os.path.exists('classify.py'):\n",
    "    print(\"✓ classify.py found\")\n",
    "else:\n",
    "    print(\"✗ classify.py not found - make sure you're running this notebook in the correct directory\")\n",
    "\n",
    "# Check if sample PDF exists\n",
    "if os.path.exists('sample_document.pdf'):\n",
    "    print(\"✓ sample_document.pdf found\")\n",
    "else:\n",
    "    print(\"✗ sample_document.pdf not found - we'll create it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Step 2: Display Help Information\n",
    "\n",
    "Let's check the available options for the classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display help information for the classify script\n",
    "result = subprocess.run(['python3', 'classify.py', '--help'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Create and Examine Sample Documents\n",
    "\n",
    "Let's create sample documents if they don't exist. The new API works with direct document upload, so no text extraction is needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents if they don't exist\n",
    "sample_dir = \"sample_documents\"\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    print(f\"Created {sample_dir} directory\")\n",
    "else:\n",
    "    print(f\"✓ {sample_dir} directory already exists\")\n",
    "\n",
    "# Create sample text file with Strong Mayor Powers reference\n",
    "sample_txt = os.path.join(sample_dir, 'comment_001.txt')\n",
    "if not os.path.exists(sample_txt):\n",
    "    with open(sample_txt, 'w') as f:\n",
    "        f.write(\"This is a comment about Strong Mayor Powers in Ontario. \"\n",
    "                \"I believe these enhanced mayoral authorities will help streamline \"\n",
    "                \"municipal decision-making processes.\")\n",
    "    print(f\"✓ Created {sample_txt}\")\n",
    "else:\n",
    "    print(f\"✓ {sample_txt} already exists\")\n",
    "\n",
    "# Create sample HTML file\n",
    "sample_html = os.path.join(sample_dir, 'comment_002.html')\n",
    "if not os.path.exists(sample_html):\n",
    "    with open(sample_html, 'w') as f:\n",
    "        f.write(\"\"\"<!DOCTYPE html>\n",
    "<html><head><title>Public Comment</title></head>\n",
    "<body>\n",
    "<h1>Municipal Governance Comment</h1>\n",
    "<p>I have concerns about the new <strong>mayor override powers</strong> \n",
    "that were recently implemented. These authorities seem too broad.</p>\n",
    "</body></html>\"\"\")\n",
    "    print(f\"✓ Created {sample_html}\")\n",
    "else:\n",
    "    print(f\"✓ {sample_html} already exists\")\n",
    "\n",
    "# Show files in sample directory\n",
    "print(\"\\nFiles in sample_documents directory:\")\n",
    "for file in os.listdir(sample_dir):\n",
    "    file_path = os.path.join(sample_dir, file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Sample Document Content\n",
    "\n",
    "Let's examine what content we have in our sample documents. Note: With the new API, we send documents directly without text extraction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display content of our sample documents\n",
    "sample_dir = \"sample_documents\"\n",
    "\n",
    "if os.path.exists(sample_dir):\n",
    "    print(\"Sample Document Contents:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for file in os.listdir(sample_dir):\n",
    "        file_path = os.path.join(sample_dir, file)\n",
    "        print(f\"\\n--- {file} ---\")\n",
    "        \n",
    "        if file.endswith('.txt'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "        elif file.endswith('.html'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "                print(content[:300] + \"...\" if len(content) > 300 else content)\n",
    "        else:\n",
    "            print(f\"Binary file ({os.path.getsize(file_path)} bytes)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"\\nNote: With the new API, these documents are sent directly to Gemini\")\n",
    "    print(\"without any text extraction preprocessing!\")\n",
    "else:\n",
    "    print(\"Sample directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 4: Run Dry-Run Mode\n",
    "\n",
    "Before making actual API calls, let's run the tool in dry-run mode to estimate costs using the new API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for our sample document to demonstrate directory processing\n",
    "# (We already created this above, but let's make sure it exists)\n",
    "sample_dir = \"sample_documents\"\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "    print(f\"Created {sample_dir} directory\")\n",
    "\n",
    "# List files in sample directory\n",
    "print(\"Files in sample_documents directory:\")\n",
    "for file in os.listdir(sample_dir):\n",
    "    print(f\"  - {file}\")\n",
    "    \n",
    "print(f\"\\nNote: The new API processes documents directly from the directory!\")\n",
    "print(f\"No JSON input files are supported anymore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dry-run mode to estimate token usage\n",
    "print(\"Running dry-run mode to estimate token usage...\\n\")\n",
    "\n",
    "result = subprocess.run(['python3', 'classify.py', sample_dir, '--dry-run'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "print(\"Dry-run output:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nErrors/Warnings:\")\n",
    "    print(result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 5: Understanding Token Estimation\n",
    "\n",
    "The dry-run mode shows us how many tokens would be used for the API call. Let's break this down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract token information from dry-run output\n",
    "import re\n",
    "\n",
    "# Parse the output to extract token count\n",
    "output_lines = result.stdout.split('\\n')\n",
    "token_line = [line for line in output_lines if 'tokens required' in line]\n",
    "\n",
    "if token_line:\n",
    "    # Extract token count using regex\n",
    "    token_match = re.search(r'(\\d+)', token_line[0])\n",
    "    if token_match:\n",
    "        estimated_tokens = int(token_match.group(1))\n",
    "        print(f\"Estimated tokens for our sample document: {estimated_tokens:,}\")\n",
    "        \n",
    "        # Provide cost estimation (approximate)\n",
    "        # Note: These are example rates - check current Gemini pricing\n",
    "        cost_per_1k_tokens = 0.00025  # Example rate for Gemini 1.5 Flash\n",
    "        estimated_cost = (estimated_tokens / 1000) * cost_per_1k_tokens\n",
    "        print(f\"Estimated cost (approximate): ${estimated_cost:.6f}\")\n",
    "        \n",
    "        print(f\"\\nNote: This is just an estimate. Actual costs may vary.\")\n",
    "        print(f\"Check current Google Gemini API pricing for accurate rates.\")\n",
    "    else:\n",
    "        print(\"Could not extract token count from output\")\n",
    "else:\n",
    "    print(\"Token information not found in dry-run output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 6: Understanding the New API Approach\n",
    "\n",
    "The new Google GenAI API uses direct document upload instead of text extraction. Let's understand what this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show how the new API works conceptually\n",
    "print(\"New Google GenAI API Approach:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Documents are uploaded directly as bytes (no text extraction)\")\n",
    "print(\"2. Uses types.Part.from_bytes() to create document parts\")\n",
    "print(\"3. API handles text extraction internally\")\n",
    "print(\"4. Structured responses using enums\")\n",
    "print(\"5. Model: gemini-2.5-flash (1M token context window)\")\n",
    "print(\"6. Environment variable: GEMINI_API_KEY\")\n",
    "print(\"\")\n",
    "print(\"Key Benefits:\")\n",
    "print(\"- No dependency on text extraction libraries\")\n",
    "print(\"- Better handling of complex document formats\")\n",
    "print(\"- More reliable text extraction by Google's models\")\n",
    "print(\"- Structured responses with enum validation\")\n",
    "print(\"\")\n",
    "print(\"Example API call structure:\")\n",
    "print(\"\")\n",
    "print(\"```python\")\n",
    "print(\"from google import genai\")\n",
    "print(\"from google.genai import types\")\n",
    "print(\"\")\n",
    "print(\"client = genai.Client()  # Uses GEMINI_API_KEY env var\")\n",
    "print(\"\")\n",
    "print(\"document_part = types.Part.from_bytes(\")\n",
    "print(\"    data=file_bytes,\")\n",
    "print(\"    mime_type='application/pdf'\")\n",
    "print(\")\")\n",
    "print(\"\")\n",
    "print(\"response = client.models.generate_content(\")\n",
    "print(\"    model='gemini-2.5-flash',\")\n",
    "print(\"    contents=[document_part, prompt],\")\n",
    "print(\"    config={\")\n",
    "print(\"        'response_mime_type': 'text/x.enum',\")\n",
    "print(\"        'response_schema': ClassificationEnum,\")\n",
    "print(\"    }\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 7: Manual Analysis\n",
    "\n",
    "Based on the content we have, let's manually analyze what we expect the tool to find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's manually identify Strong Mayor Powers references in our sample documents\n",
    "sample_dir = \"sample_documents\"\n",
    "\n",
    "if os.path.exists(sample_dir):\n",
    "    print(\"Manual analysis of Strong Mayor Powers references:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key phrases to look for\n",
    "    strong_mayor_phrases = [\n",
    "        \"Strong Mayor Powers\",\n",
    "        \"enhanced mayoral authorities\", \n",
    "        \"override certain council decisions\",\n",
    "        \"enhanced control over municipal planning\",\n",
    "        \"mayor override powers\",\n",
    "        \"mayoral authorities\",\n",
    "        \"streamlined decision-making capabilities\"\n",
    "    ]\n",
    "    \n",
    "    for file in os.listdir(sample_dir):\n",
    "        file_path = os.path.join(sample_dir, file)\n",
    "        print(f\"\\n--- Analyzing {file} ---\")\n",
    "        \n",
    "        if file.endswith('.txt'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "        elif file.endswith('.html'):\n",
    "            with open(file_path, 'r') as f:\n",
    "                content = f.read()\n",
    "        else:\n",
    "            print(\"Binary file - would be processed directly by API\")\n",
    "            continue\n",
    "            \n",
    "        found_phrases = []\n",
    "        for phrase in strong_mayor_phrases:\n",
    "            if phrase.lower() in content.lower():\n",
    "                found_phrases.append(phrase)\n",
    "                # Find the context around this phrase\n",
    "                content_lower = content.lower()\n",
    "                phrase_lower = phrase.lower()\n",
    "                index = content_lower.find(phrase_lower)\n",
    "                if index != -1:\n",
    "                    # Get 50 characters before and after the phrase\n",
    "                    start = max(0, index - 50)\n",
    "                    end = min(len(content), index + len(phrase) + 50)\n",
    "                    context = content[start:end].replace('\\n', ' ')\n",
    "                    print(f\"  ✓ Found: '{phrase}'\")\n",
    "                    print(f\"    Context: ...{context}...\")\n",
    "        \n",
    "        print(f\"\\n  Summary for {file}: Found {len(found_phrases)} references\")\n",
    "        print(f\"  Expected classification: {'PRESENT' if found_phrases else 'ABSENT'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"\\nOverall Analysis:\")\n",
    "    print(\"Our sample documents contain clear references to Strong Mayor Powers\")\n",
    "    print(\"The tool should classify both as 'present'\")\n",
    "else:\n",
    "    print(\"Sample directory not available for manual analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Step 8: Running with API Key (Optional)\n",
    "\n",
    "If you have a Gemini API key, you can run the actual classification using the new API. Otherwise, we'll show what the expected output would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API key is available\n",
    "api_key = os.environ.get('GEMINI_API_KEY')\n",
    "\n",
    "if api_key:\n",
    "    print(\"✓ Gemini API key found in environment variables\")\n",
    "    print(\"Running actual classification with new Google GenAI API...\\n\")\n",
    "    \n",
    "    # Run the actual classification\n",
    "    result = subprocess.run(['python3', 'classify.py', sample_dir, \n",
    "                           '--output-csv', 'demo_results.csv'], \n",
    "                           capture_output=True, text=True)\n",
    "    \n",
    "    print(\"Classification output:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"\\nErrors/Warnings:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Check if results file was created\n",
    "    if os.path.exists('demo_results.csv'):\n",
    "        print(\"\\n✓ Results file created: demo_results.csv\")\n",
    "    else:\n",
    "        print(\"\\n✗ Results file not created\")\n",
    "        \n",
    "else:\n",
    "    print(\"✗ No Gemini API key found in GEMINI_API_KEY environment variable\")\n",
    "    print(\"\\nTo run the actual classification, you would need to:\")\n",
    "    print(\"1. Get a Gemini API key from Google AI Studio\")\n",
    "    print(\"2. Set it as an environment variable: export GEMINI_API_KEY='your-key-here'\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "    print(\"\\nNote: The new API uses GEMINI_API_KEY (not GOOGLE_API_KEY)\")\n",
    "    print(\"For demonstration purposes, we'll show expected output format below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Step 9: Examining Results\n",
    "\n",
    "Let's look at the results file format and interpret the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have actual results or create expected results for demo\n",
    "results_file = 'demo_results.csv'\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    # Read actual results\n",
    "    print(\"Reading actual classification results:\")\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"\\nResults Analysis:\")\n",
    "    print(f\"Total documents processed: {len(df)}\")\n",
    "    \n",
    "    if 'Strong Mayor Powers' in df.columns:\n",
    "        value_counts = df['Strong Mayor Powers'].value_counts()\n",
    "        print(f\"Classification results:\")\n",
    "        for classification, count in value_counts.items():\n",
    "            print(f\"  - {classification}: {count}\")\n",
    "            \n",
    "else:\n",
    "    # Create expected results for demonstration\n",
    "    print(\"Expected results format (since no API key was provided):\")\n",
    "    \n",
    "    expected_results = pd.DataFrame({\n",
    "        'Comment ID': ['comment_001'],\n",
    "        'Strong Mayor Powers': ['present']\n",
    "    })\n",
    "    \n",
    "    print(expected_results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nExpected Analysis:\")\n",
    "    print(\"Total documents processed: 1\")\n",
    "    print(\"Classification results:\")\n",
    "    print(\"  - present: 1\")\n",
    "    print(\"\\nThis matches our manual analysis - the document contains multiple\")\n",
    "    print(\"references to Strong Mayor Powers and should be classified as 'present'.\")\n",
    "    \n",
    "    # Save expected results for demo purposes\n",
    "    expected_results.to_csv('expected_results.csv', index=False)\n",
    "    print(\"\\n✓ Expected results saved to expected_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Step 10: Visualizing Results (if we have them)\n",
    "\n",
    "Let's create a simple visualization of the classification results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use actual or expected results\n",
    "if os.path.exists('demo_results.csv'):\n",
    "    df = pd.read_csv('demo_results.csv')\n",
    "    title_suffix = \"(Actual Results)\"\n",
    "elif os.path.exists('expected_results.csv'):\n",
    "    df = pd.read_csv('expected_results.csv')\n",
    "    title_suffix = \"(Expected Results)\"\n",
    "else:\n",
    "    # Fallback\n",
    "    df = pd.DataFrame({\n",
    "        'Comment ID': ['comment_001'],\n",
    "        'Strong Mayor Powers': ['present']\n",
    "    })\n",
    "    title_suffix = \"(Demo)\"\n",
    "\n",
    "# Create a pie chart of classifications\n",
    "if 'Strong Mayor Powers' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Subplot 1: Pie chart\n",
    "    plt.subplot(1, 2, 1)\n",
    "    value_counts = df['Strong Mayor Powers'].value_counts()\n",
    "    colors = ['#ff7f7f' if x == 'present' else '#7f7fff' for x in value_counts.index]\n",
    "    plt.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "    plt.title(f'Strong Mayor Powers Classifications {title_suffix}')\n",
    "    \n",
    "    # Subplot 2: Bar chart\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(value_counts.index, value_counts.values, color=colors)\n",
    "    plt.title('Classification Counts')\n",
    "    plt.ylabel('Number of Documents')\n",
    "    plt.xlabel('Classification')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVisualization shows the distribution of classifications.\")\n",
    "    print(\"Red/pink = 'present' (contains Strong Mayor Powers references)\")\n",
    "    print(\"Blue = 'absent' (no Strong Mayor Powers references)\")\n",
    "else:\n",
    "    print(\"No classification data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Step 11: Understanding Different Input Formats\n",
    "\n",
    "The tool supports various input formats. Let's demonstrate how it works with different file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples of different file types\n",
    "print(\"Creating sample files in different formats...\\n\")\n",
    "\n",
    "# Text file\n",
    "with open(os.path.join(sample_dir, 'comment_002.txt'), 'w') as f:\n",
    "    f.write(\"This is a simple text comment about municipal governance. \"\n",
    "            \"I think the new Strong Mayor Powers are concerning.\")\n",
    "print(\"✓ Created comment_002.txt\")\n",
    "\n",
    "# HTML file\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head><title>Public Comment</title></head>\n",
    "<body>\n",
    "<h1>Municipal Governance Comment</h1>\n",
    "<p>I support the implementation of <strong>enhanced mayoral authorities</strong> \n",
    "as they will help streamline decision-making processes in our municipality.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "with open(os.path.join(sample_dir, 'comment_003.html'), 'w') as f:\n",
    "    f.write(html_content)\n",
    "print(\"✓ Created comment_003.html\")\n",
    "\n",
    "# List all files in the sample directory\n",
    "print(\"\\nAll sample files created:\")\n",
    "for file in sorted(os.listdir(sample_dir)):\n",
    "    file_path = os.path.join(sample_dir, file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Step 12: Running Dry-Run on Multiple Files\n",
    "\n",
    "Let's see how the tool handles multiple files of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dry-run on the expanded sample directory\n",
    "print(\"Running dry-run on multiple file types...\\n\")\n",
    "\n",
    "result = subprocess.run(['python3', 'classify.py', sample_dir, '--dry-run'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "print(\"Dry-run output for multiple files:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nErrors/Warnings:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "# Extract and analyze token information\n",
    "output_lines = result.stdout.split('\\n')\n",
    "\n",
    "# Find the line with total comments\n",
    "total_comments_line = [line for line in output_lines if 'total of' in line and 'comments' in line]\n",
    "if total_comments_line:\n",
    "    print(f\"\\nProcessed files: {total_comments_line[0]}\")\n",
    "\n",
    "# Find token usage\n",
    "token_line = [line for line in output_lines if 'tokens required' in line]\n",
    "if token_line:\n",
    "    print(f\"Token usage: {token_line[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Step 13: Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've learned and provide guidance for real-world usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DEMO SUMMARY: Strong Mayor Powers Classification Tool (New Google GenAI API)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ What we demonstrated:\")\n",
    "print(\"  1. Tool setup with new Google GenAI API\")\n",
    "print(\"  2. Sample document creation (multiple formats)\")\n",
    "print(\"  3. Direct document upload (no text extraction)\")\n",
    "print(\"  4. Dry-run mode for cost estimation\")\n",
    "print(\"  5. Understanding the new API approach\")\n",
    "print(\"  6. Manual analysis of Strong Mayor Powers references\")\n",
    "print(\"  7. Structured responses using enums\")\n",
    "print(\"  8. Results interpretation\")\n",
    "\n",
    "print(\"\\n📊 Key findings from our samples:\")\n",
    "print(\"  - Sample documents contain Strong Mayor Powers references\")\n",
    "print(\"  - Expected classification: PRESENT for both documents\")\n",
    "print(\"  - Tool processes multiple file formats directly\")\n",
    "print(\"  - No more text extraction dependencies\")\n",
    "\n",
    "print(\"\\n🔧 For real-world usage:\")\n",
    "print(\"  1. Obtain Gemini API key from Google AI Studio\")\n",
    "print(\"  2. Set environment variable: export GEMINI_API_KEY='your-key'\")\n",
    "print(\"  3. Prepare documents in a directory (no JSON files)\")\n",
    "print(\"  4. Run dry-run first to estimate costs\")\n",
    "print(\"  5. Process documents and analyze results\")\n",
    "\n",
    "print(\"\\n📁 Supported file formats:\")\n",
    "print(\"  - PDF (.pdf) - direct upload, no pdfplumber needed\")\n",
    "print(\"  - Text (.txt, .text) - direct upload\")\n",
    "print(\"  - HTML (.html, .htm) - direct upload, no BeautifulSoup needed\")\n",
    "print(\"  - Word (.docx) - direct upload, no python-docx needed\")\n",
    "print(\"  - RTF (.rtf) - direct upload, no striprtf needed\")\n",
    "print(\"  - JSON input is NO LONGER SUPPORTED\")\n",
    "\n",
    "print(\"\\n🚀 New API Benefits:\")\n",
    "print(\"  - Simpler dependency management\")\n",
    "print(\"  - Better document processing by Google's models\")\n",
    "print(\"  - Structured responses with enum validation\")\n",
    "print(\"  - Updated model: gemini-2.5-flash (1M token context)\")\n",
    "print(\"  - More reliable text extraction\")\n",
    "\n",
    "print(\"\\n💡 Migration notes:\")\n",
    "print(\"  - Environment variable changed: GOOGLE_API_KEY → GEMINI_API_KEY\")\n",
    "print(\"  - Argument changed: --google-api-key → --gemini-api-key\")\n",
    "print(\"  - JSON input removed, directory input only\")\n",
    "print(\"  - No more text extraction libraries needed\")\n",
    "print(\"  - Model updated to gemini-2.5-flash\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "If you want to clean up the demo files, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup demo files (uncomment to run)\n",
    "# import shutil\n",
    "# \n",
    "# cleanup_files = [\n",
    "#     'sample_documents',\n",
    "#     'demo_results.csv', \n",
    "#     'expected_results.csv',\n",
    "#     'sample_document.pdf'\n",
    "# ]\n",
    "# \n",
    "# print(\"Cleaning up demo files...\")\n",
    "# for item in cleanup_files:\n",
    "#     if os.path.exists(item):\n",
    "#         if os.path.isdir(item):\n",
    "#             shutil.rmtree(item)\n",
    "#             print(f\"✓ Removed directory: {item}\")\n",
    "#         else:\n",
    "#             os.remove(item)\n",
    "#             print(f\"✓ Removed file: {item}\")\n",
    "#     else:\n",
    "#         print(f\"  - {item} (not found)\")\n",
    "# \n",
    "# print(\"\\nCleanup complete!\")\n",
    "\n",
    "print(\"Cleanup cell is commented out. Uncomment and run to clean up demo files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}